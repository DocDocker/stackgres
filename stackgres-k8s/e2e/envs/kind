#!/bin/sh

KIND_NAME="${KIND_NAME:-kind}"
KIND_NODES="${KIND_NODES:-1}"

export KIND_NAME
export KIND_NODES

check_kind_version() {
  kind version | grep -q -F 'kind v0.7.0 '
}

update_k8s_config() {
  check_kind_version

  if cat /proc/1/cgroup | grep :memory | grep -E ":/$" > /dev/null
  then 
    echo "Running outside of a container"
    return
  else 
    echo "Running inside of a container"
  fi  
  mkdir -p "$HOME/.kube"
  if [ "$K8S_FROM_DIND" = true ]
  then
    if docker network ls --format '{{ .Name }}' | grep -q '^kind$'
    then
      local CONTAINER_NAME="$(docker ps --format '{{ .Names }}' | grep "^$(hostname)")"
      docker inspect "$CONTAINER_NAME" \
        -f '{{ range $key,$value := .NetworkSettings.Networks }}{{ printf "%s\n" $key }}{{ end }}' \
        | grep -q '^kind$' \
        || docker network connect kind "$CONTAINER_NAME"
    fi
    local KIND_CONTROL_PLANE_IP="$(docker inspect "$KIND_NAME-control-plane" \
      -f '{{ .NetworkSettings.Networks.kind.IPAddress }}')"
    kind get kubeconfig --name "$KIND_NAME" --internal \
      | sed "s/$KIND_NAME-control-plane/$KIND_CONTROL_PLANE_IP/" \
      > "$HOME/.kube/config-$KIND_NAME"
  else
    kind get kubeconfig --name "$KIND_NAME" \
      > "$HOME/.kube/config-$KIND_NAME"
  fi

  (
  export KUBECONFIG="${KUBECONFIG:-$HOME/.kube/config}"
  if [ -s "$KUBECONFIG" ]
  then
    KUBECONFIG="$HOME/.kube/config-$KIND_NAME":"$KUBECONFIG" \
      kubectl config view --raw > "$HOME/.kube/config-merged"
    mv "$HOME/.kube/config-merged" "$KUBECONFIG"
  else
    mv "$HOME/.kube/config-$KIND_NAME" "$KUBECONFIG"
  fi
  )
}

reuse_k8s() {
  check_kind_version
  check_kind_image_exists

  if ! kind get clusters | grep -q "^$KIND_NAME$" \
      || ! docker inspect "$KIND_NAME-control-plane" -f '{{ .State.Status }}' \
        | grep -q -F 'running' \
      || ! docker inspect "$KIND_NAME-control-plane" -f '{{ .Config.Image }}' \
        | grep -q -F "kindest/node:v$(get_kind_image "${K8S_VERSION}")"
  then
    echo "Can not reuse kind environment $KIND_NAME"
    reset_k8s
    return
  fi

  echo "Reusing kind environment $KIND_NAME"

  update_k8s_config
}

reset_k8s() {
  check_kind_version

  echo "Setting up kind environment $KIND_NAME..."

  kind delete cluster --name "$KIND_NAME" || true
  cat << EOF > "$TARGET_PATH/kind-config.yaml"
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
$(
  if [ "${K8S_VERSION%.*}" = 1.11 ]
  then
    cat << INNER_EOF
kubeadmConfigPatches:
- |
  apiVersion: kubeadm.k8s.io/v1alpha2
  kind: MasterConfiguration
  kubernetesVersion: v1.11.10
  metadata:
    name: config
  apiServerExtraArgs:
    "feature-gates": "DynamicProvisioningScheduling=true"
  schedulerExtraArgs:
    "feature-gates": "DynamicProvisioningScheduling=true"
  controllerManagerExtraArgs:
    "feature-gates": "DynamicProvisioningScheduling=true"
INNER_EOF
  fi
)
networking:
  apiServerAddress: "0.0.0.0"
nodes:
- role: control-plane
$(
  if [ -n "$K8S_EXTRA_PORT" ]
  then
    cat << INNER_EOF
  extraPortMappings:
  - containerPort: $(echo "$K8S_EXTRA_PORT" | cut -d : -f 1)
    hostPort: $(echo "$K8S_EXTRA_PORT" | cut -d : -f 2)
    listenAddress: "$(echo "$K8S_EXTRA_PORT" | cut -d : -f 3)"
    protocol: "$(echo "$K8S_EXTRA_PORT" | cut -d : -f 4)"
INNER_EOF
  fi
  for KIND_NODE in $(seq 2 "$KIND_NODES")
  do
    cat << INNER_EOF
- role: worker
INNER_EOF
  done
)
EOF

  kind create cluster --name "$KIND_NAME" --config "$TARGET_PATH/kind-config.yaml" \
    --image "kindest/node:v$(get_kind_image "${K8S_VERSION}")"

  if [ "$KIND_INSTALL_NFS" = "true" ]
  then
    local pids
  
    kind get nodes --name "$KIND_NAME" \
      | xargs -r -n 1 -I % -P 0 sh -ec "
      docker exec -t '%' sh -c 'DEBIAN_FRONTEND=noninteractive apt-get update -y -qq < /dev/null > /dev/null'
      docker exec -t '%' sh -c 'DEBIAN_FRONTEND=noninteractive apt-get install -y -qq nfs-common < /dev/null > /dev/null'
      "
  fi

  update_k8s_config

  if [ "${K8S_VERSION%.*}" = 1.12 ]
  then
    # Patch coredns to version 1.3.1 (see https://github.com/coredns/coredns/issues/2391)
    kubectl patch deployment -n kube-system coredns --type json \
      --patch '[{"op":"replace","path":"/spec/template/spec/containers/0/image","value":"k8s.gcr.io/coredns:1.3.1"}]'
  fi

  if [ "${K8S_VERSION%.*}" = 1.11 ]
  then
    set_local_path_storage_as_default
  fi

  echo "...done"
}

delete_k8s() {
  check_kind_version

  echo "Deleting kind environment $KIND_NAME..."

  kind delete cluster --name "$KIND_NAME" || true

  rm -f "$(kind get kubeconfig-path --name "$KIND_NAME")"

  echo "...done"
}

load_image_k8s() {
  check_kind_version
  echo "Loading image $1 in kind environemnt $KIND_NAME..."

  kind load docker-image --name "$KIND_NAME" "$1"

  echo "...done"
}

load_certificate_k8s() {
  check_kind_version

  echo "Loading certificate $1 in kind environemnt $KIND_NAME..."

    kind get nodes --name "$KIND_NAME" \
      | xargs -r -n 1 -I % -P 0 sh -ec "
      docker cp '$1' '%':/usr/local/share/ca-certificates/validator.crt
      docker exec -t '%' sh -c "update-ca-certificates"
      "

  echo "...done"
}

excluded_namespaces() {
  echo 'local-path-storage'
}

get_k8s_versions() {
  get_kind_images | cut -d @ -f 1 | sed 's/^v//'
}

check_kind_image_exists() {
  try_function get_kind_image "${K8S_VERSION}" > /dev/null 2>&1
  if ! "$RESULT"
  then
    echo "Kind image for k8s version ${K8S_VERSION} not found."
    echo "Available images exists only for versions: $(get_k8s_versions)"
    exit 1
  fi
}

get_kind_image() {
  get_kind_images | sed 's/^v//' | grep "^$1@"
}

get_kind_images() {
  cat << EOF
v1.18.0@sha256:0e20578828edd939d25eb98496a685c76c98d54084932f76069f886ec315d694
v1.17.0@sha256:9512edae126da271b66b990b6fff768fbb7cd786c7d39e86bdf55906352fdf62
v1.16.4@sha256:b91a2c2317a000f3a783489dfb755064177dbc3a0b2f4147d50f04825d016f55
v1.15.7@sha256:e2df133f80ef633c53c0200114fce2ed5e1f6947477dbc83261a6a921169488d
v1.14.10@sha256:81ae5a3237c779efc4dda43cc81c696f88a194abcc4f8fa34f86cf674aa14977
v1.13.12@sha256:5e8ae1a4e39f3d151d420ef912e18368745a2ede6d20ea87506920cd947a7e3a
v1.12.10@sha256:68a6581f64b54994b824708286fafc37f1227b7b54cbb8865182ce1e036ed1cc
v1.11.10@sha256:e6f3dade95b7cb74081c5b9f3291aaaa6026a90a977e0b990778b6adc9ea6248
EOF
}

set_local_path_storage_as_default() {
  kubectl patch storageclasses.storage.k8s.io standard --type='json' \
    --patch '[{"op": "remove", "path": "/metadata/annotations/storageclass.kubernetes.io~1is-default-class"}]'
  cat << 'EOF' | kubectl create -f -
apiVersion: v1
kind: Namespace
metadata:
  name: local-path-storage
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: local-path-provisioner-service-account
  namespace: local-path-storage
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRole
metadata:
  name: local-path-provisioner-role
  namespace: local-path-storage
rules:
- apiGroups: [""]
  resources: ["nodes", "persistentvolumeclaims"]
  verbs: ["get", "list", "watch"]
- apiGroups: [""]
  resources: ["endpoints", "persistentvolumes", "pods"]
  verbs: ["*"]
- apiGroups: [""]
  resources: ["events"]
  verbs: ["create", "patch"]
- apiGroups: ["storage.k8s.io"]
  resources: ["storageclasses"]
  verbs: ["get", "list", "watch"]
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: local-path-provisioner-bind
  namespace: local-path-storage
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: local-path-provisioner-role
subjects:
- kind: ServiceAccount
  name: local-path-provisioner-service-account
  namespace: local-path-storage
---
apiVersion: apps/v1beta2
kind: Deployment
metadata:
  name: local-path-provisioner
  namespace: local-path-storage
spec:
  replicas: 1
  selector:
    matchLabels:
      app: local-path-provisioner
  template:
    metadata:
      labels:
        app: local-path-provisioner
    spec:
      serviceAccountName: local-path-provisioner-service-account
      containers:
      - name: local-path-provisioner
        image: rancher/local-path-provisioner:v0.0.1
        imagePullPolicy: Always
        command:
        - local-path-provisioner
        - --debug
        - start
        - --config
        - /etc/config/config.json
        volumeMounts:
        - name: config-volume
          mountPath: /etc/config/
        env:
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
      volumes:
        - name: config-volume
          configMap:
            name: local-path-config
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: local-path
  annotations:
    storageclass.kubernetes.io/is-default-class: "true"
provisioner: rancher.io/local-path
volumeBindingMode: WaitForFirstConsumer
reclaimPolicy: Delete
---
kind: ConfigMap
apiVersion: v1
metadata:
  name: local-path-config
  namespace: local-path-storage
data:
  config.json: |-
        {
                "nodePathMap":[
                {
                        "node":"DEFAULT_PATH_FOR_NON_LISTED_NODES",
                        "paths":["/opt/local-path-provisioner"]
                }
                ]
        }
EOF
}
